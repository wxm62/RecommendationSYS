import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import tqdm
import numpy as np
import time
import psutil


# ç”Ÿæˆæ‹“æ‰‘å›¾ï¼Œä¸‰ä¸ªå‚æ•°
def get_Graph3(filename):
    file_name = filename
    g = nx.Graph()
    with open(file_name) as file:
        for line in file:
            head, tail, time = [str(x) for x in line.split()]  # å¦‚æœèŠ‚ç‚¹ä½¿ç”¨æ•°ç»„è¡¨ç¤ºçš„å¯ä»¥å°†str(x)æ”¹ä¸ºint( x)
            g.add_edge(head, tail)
        # nx.draw(g, node_color='yellow', with_labels=True, node_size=800, alpha=0.95, font_size=10)
        # plt.show()
        # print(head, tail)
    return g


# è¾“å‡ºç»˜åˆ¶æ‹“æ‰‘å›¾
def print_graph(G):
    plt.figure()  # åˆ›å»ºä¸€å¹…å›¾
    # è¾“å‡ºç½‘ç»œæ‹“æ‰‘å›¾
    nx.draw(G, node_color='yellow', with_labels=True, node_size=800, alpha=0.95, font_size=10)
    # print(G.nodes())
    plt.show()


# å¾—åˆ°æŸä¸ªèŠ‚ç‚¹çš„åº¦å€¼
def gDegree(G, nodeid):
    """
    å°†G.degree()çš„è¿”å›å€¼å˜ä¸ºå­—å…¸
    """
    node_degrees_dict = {}
    for i in G.degree():
        node_degrees_dict[i[0]] = i[1]
    default_info = 'è¯¥èŠ‚ç‚¹ç¼–å·ä¸å­˜åœ¨'
    result = node_degrees_dict.get(nodeid, default_info)
    return result


# Kå£³åˆ†è§£ï¼ŒæŒ‰ç…§kså€¼
def kshell(G):
    ks_results = {}
    DF = pd.DataFrame()
    # print(G)
    for num in range(1, 18):
        ks = nx.k_shell(G, num)  # ç¬¬äºŒå±‚èŠ‚ç‚¹
        # if not ks:
        #     break

        # ks_results.update(key=num, value=ks.nodes())
        ks_results[num] = ks.nodes()
        print(ks.nodes())
    for i in range(1, 18):
        print("ks={}:{}".format(i, ks_results[i]))
        k_label = pd.Series([i, ks_results[i]])
        k_label.index = ('kshell', 'result')
        DF = DF.append(k_label, ignore_index=True)
    # print("ks=1:{}".format(ks_results[1]))
    DF.to_csv("result-kshell.csv", index=False)


# Kå£³åˆ†è§£ï¼ŒæŒ‰ç…§èŠ‚ç‚¹
def kshell2(G):
    ks_results = {}
    DF = pd.DataFrame()

    # print(G)
    for num in range(1, 18):
        ks = nx.k_shell(G, num)  # ç¬¬numå±‚çš„å­å›¾
        # if not ks:
        #     break
        # ks_results.update(key=num, value=ks.nodes())
        ks_results[num] = ks.nodes()
    for i in range(1, 18):
        for value in ks_results[i]:
            k_label = pd.Series([value, i])
            k_label.index = ('nodeid', 'kshell')
            DF = DF.append(k_label, ignore_index=True)
    DF.to_csv("result-kshell2.csv", index=False)


# æ’åºå’Œå½’ä¸€åŒ–
def sort_kshell():
    df1 = pd.read_csv('result-kshell2.csv')
    df1 = df1.sort_values(by=['nodeid'], ascending=True)
    # df2 = pd.read_csv('result_bc.csv')
    DF1 = pd.DataFrame()
    DF1['nodeid'] = df1['nodeid']
    DF1['kshell'] = df1['kshell']
    # ä¿ç•™å…­ä½å°æ•°
    # DF1['kshell_guiyihua'] = round(((df1['kshell'] - 1) / (34 - 1)), 6)
    # ä¸ä¿ç•™å…­ä½å°æ•°
    DF1['kshell_guiyihua'] = ((df1['kshell'] - 1) / (17 - 1))
    DF1.to_csv("result_sort_kshell.csv", index=False)


# è®¡ç®—SaltonæŒ‡æ ‡
def get_Salton(G):
    DF = pd.DataFrame()
    for i in G.nodes():
        for j in G.nodes():
            # æºèŠ‚ç‚¹åˆ°æºèŠ‚ç‚¹æ²¡æœ‰åº¦,ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰å…±åŒé‚»å±…ï¼Œåœæ­¢
            if i != j:
                if gDegree(G, str(i)) >= 1 and gDegree(G, str(i)) >= 1:
                    if nx.common_neighbors(G, str(i), str(j)):
                        if len(list(nx.common_neighbors(G, str(i), str(j)))) != 0:
                            cneighbor = len(list(nx.common_neighbors(G, str(i), str(j))))
                            degreesum = pow((gDegree(G, str(i)) * gDegree(G, str(j))), 0.5)
                            result = cneighbor / degreesum
                            salton_label = pd.Series([i, j, result])
                            salton_label.index = ('node1', 'node2', 'result')
                            DF = DF.append(salton_label, ignore_index=True)
                            # salton.append(result)
                            # print(salton)
                            print("èŠ‚ç‚¹{}å’ŒèŠ‚ç‚¹{}çš„saltonæŒ‡æ ‡å€¼ï¼š{},å…±åŒé‚»å±…ä¸ªæ•°ä¸ºï¼š{},åº¦ä¹‹å’Œçš„æ ¹å·ä¸ºï¼š{}".format(i, j, result, cneighbor,
                                                                                        degreesum))
                            print("DF:", DF)
                else:
                    break
    DF.to_csv("result_salton.csv", index=False)


# ä¸€ä¸ªèŠ‚ç‚¹çš„èŠ‚ç‚¹åº¦è¶Šå¤§å°±æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹çš„åº¦ä¸­å¿ƒæ€§è¶Šé«˜ï¼Œè¯¥èŠ‚ç‚¹åœ¨ç½‘ç»œä¸­å°±è¶Šé‡è¦ã€‚
def centrality(G):
    # è®¡ç®—åº¦ä¸­å¿ƒæ€§,é™åº
    dc = nx.algorithms.centrality.degree_centrality(G)
    return sorted(dc.items(), key=lambda x: x[1], reverse=True)


# ä»¥ç»è¿‡æŸä¸ªèŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„æ•°ç›®æ¥åˆ»ç”»èŠ‚ç‚¹é‡è¦æ€§çš„æŒ‡æ ‡
def betweenness(G, id):
    # è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§,é™åº
    dc = nx.betweenness_centrality(G)
    # print(dc[id])
    return dc[id]
    # return sorted(dc.items(), key=lambda x: x[1], reverse=True)


# åæ˜ åœ¨ç½‘ç»œä¸­æŸä¸€èŠ‚ç‚¹ä¸å…¶ä»–èŠ‚ç‚¹ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦ã€‚å°†ä¸€ä¸ªèŠ‚ç‚¹åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„è·ç¦»çš„ç´¯åŠ èµ·æ¥çš„å€’æ•°è¡¨ç¤ºæ¥è¿‘æ€§ä¸­å¿ƒæ€§ã€‚å³å¯¹äºä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®ƒè·ç¦»å…¶ä»–èŠ‚ç‚¹è¶Šè¿‘ï¼Œé‚£ä¹ˆå®ƒçš„æ¥è¿‘æ€§ä¸­å¿ƒæ€§è¶Šå¤§ã€‚
def closeness(G):
    # è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§,é™åº
    dc = nx.closeness_centrality(G)
    return sorted(dc.items(), key=lambda x: x[1], reverse=True)


def bc_cc(G):
    DF1 = pd.DataFrame()
    DF2 = pd.DataFrame()
    DF3 = pd.DataFrame()
    BC = nx.betweenness_centrality(G)  # [('978', 0.07738095238095238), ('4', 0.023384353741496597)]
    BC = sorted(BC.items(), key=lambda x: int(x[0]), reverse=False)  # è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§,é™åº
    print(BC)
    for bc in BC:
        bc_label = pd.Series([bc[0], bc[1]])
        bc_label.index = ('node', 'BC')
        DF1 = DF1.append(bc_label, ignore_index=True)
    DF1.to_csv("result_bc.csv", index=False)

    CC = nx.closeness_centrality(G)
    CC = sorted(CC.items(), key=lambda x: int(x[0]), reverse=False)
    for cc in CC:
        cc_label = pd.Series([cc[0], cc[1]])
        cc_label.index = ('node', 'CC')
        DF2 = DF2.append(cc_label, ignore_index=True)
    DF2.to_csv("result_cc.csv", index=False)


def neighbours_bc(G):
    df = pd.read_csv('result_bc.csv')
    DF = pd.DataFrame()
    tar = []
    all_bc = []
    sum_bc = 0

    for row in df.itertuples():
        # è·å–ç¬¬ä¸€åˆ—çš„æ•°å€¼
        tar = getattr(row, 'node')
        print(tar)
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…
        nodes = list(nx.neighbors(G, str(tar)))
        for i in nodes:
            sum_bc = sum_bc + betweenness(G, i)
        all_bc.append(sum_bc)
        bizhi_label = pd.Series([int(tar), sum_bc])
        bizhi_label.index = ('node', 'neighbour_bc')
        DF = DF.append(bizhi_label, ignore_index=True)
        sum_bc = 0
    DF.to_csv("result_neighbours_bc.csv", index=False)
    print('neighbours_bcä¿å­˜æˆåŠŸ')


def importance_node():
    df1 = pd.read_csv('result_sort_kshell.csv')
    df2 = pd.read_csv('result_neighbours_bc.csv')
    df3 = pd.read_csv('result_salton.csv')
    df4 = pd.read_csv('result_bc.csv')
    df5 = pd.read_csv('result_cc.csv')
    all_salton = []
    DF = pd.DataFrame()
    DF2 = pd.DataFrame()
    DF['nodeid'] = df1['nodeid']

    for row in df1.itertuples():
        # è·å–ç¬¬ä¸€åˆ—çš„æ•°å€¼
        tar = getattr(row, 'nodeid')
        i = df3[df3['node1'] == tar]['result']
        sum = i.sum()
        all_salton.append(sum)
        sum = 0
    print(all_salton)
    print(len(all_salton))

    for k in range(0, 89):
        salton_label = pd.Series([int(k), all_salton[k]])
        salton_label.index = ('nodeid', 'nodesalton')
        DF2 = DF2.append(salton_label, ignore_index=True)
    DF2['nodeid'] = df1['nodeid']
    DF2.to_csv("result_nodesalton.csv", index=False)
    print('è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹saltonæŒ‡æ ‡ç»“æŸ')


def zhenghe_data():
    DF = pd.DataFrame()
    df1 = pd.read_csv('result_sort_kshell.csv')
    df2 = pd.read_csv('result_neighbours_bc.csv')
    df3 = pd.read_csv('result_bc.csv')
    df4 = pd.read_csv('result_cc.csv')
    df5 = pd.read_csv('result_nodesalton.csv')
    DF['node'] = df1['nodeid']
    DF['kshell'] = round(df1['kshell_guiyihua'], 6)
    DF['quanzhong'] = round((df2['neighbour_bc'] * df3['BC']) / (df4['CC'] * df4['CC']), 6)
    DF['salton'] = round(df5['nodesalton'], 6)
    DF['import'] = round(DF['kshell'] + DF['quanzhong'] * DF['salton'], 6)
    DF.to_csv("importance_dept3.csv", index=False)


# å…ˆåˆ é™¤ ğ¾ğ‘  = 0ä¸”SaltonæŒ‡æ ‡ä¸º0çš„èŠ‚ç‚¹ï¼Œå†åˆ é™¤æƒé‡ä¸ºé›¶çš„èŠ‚ç‚¹
def shuju_quzao():
    df = pd.read_csv('importance_dept3.csv')
    qu_kshell = df[(df['kshell'] != 0.0)]  # 86
    qu_salton = qu_kshell[(qu_kshell['salton'] != 0.0)]  # 86
    qu_quanzhong = qu_salton[(qu_salton['quanzhong'] != 0.0)]  # 85
    # new_df = df[df.loc['kshell'] != 0.0].dropna() & df[df.loc['salton'] != 0.0].dropna()

    print(qu_quanzhong)
    qu_quanzhong.to_csv('result_yanjiuquzao.csv', index=False)

# æ•°æ®å»å™ªçš„æ ¸å¿ƒèŠ‚ç‚¹
# def core_fringe_node():
#     df = pd.read_csv('result_yanjiuquzao.csv')
#     for i in range(0, 90):
#         df['import'] = round(df['kshell'] + df['quanzhong'] * df['salton'], 6)
#
#     df.to_csv('result_yanjiuquzao.csv', index=False)
#     print("è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ç»“æŸ")
#     df = df.sort_values(by=['import'], ascending=False)
#     df2 = df.head(17)  # å‰20% 17 67
#     df2.to_csv('corenodes.csv', index=False)
#     print("è·å¾—æ ¸å¿ƒèŠ‚ç‚¹")
#     df3 = df.tail(67)
#     df3.to_csv('fringenodes.csv', index=False)


def core_fringe_node():
    df = pd.read_csv('importance_dept3é‡è¦æ€§ä»å¤§åˆ°å°.csv')
    df2 = df.head(41)  # å‰20% 17 67
    df2.to_csv('corenodes3.csv', index=False)
    print("è·å¾—æ ¸å¿ƒèŠ‚ç‚¹")
    df3 = df.tail(48)
    df3.to_csv('fringenodes3.csv', index=False)

# åˆ†è¾¨ç‡æŒ‡æ ‡,æ— é‡å¤å€¼
def Monotonicity():
    df = pd.read_csv('result_yanjiuquzao.csv')
    output = []
    similar = 0
    for i in ():
        li = [i]
        tar = df[df['import'].isin(li)]
        sum = len(tar)
        output.append(sum)
    print(output)
    print('----')
    for j in range(0, 2):
        tar = output[j] * (output[j] - 1)
        similar = similar + tar
        # print(tar)
    print(similar)
    result = pow((1 - (similar / (112 * 111))), 2)
    print(result)  # 1


def attribute(G):
    df = pd.read_csv('result_degree.csv')
    result = 0
    for row in df.itertuples():
        # è·å–ç¬¬ä¸€åˆ—çš„æ•°å€¼
        tar = getattr(row, 'nodeid')
        result = result + nx.clustering(G, str(tar))
    print("email-dept3ç½‘ç»œçš„èŠ‚ç‚¹èšç±»ç³»æ•°å’Œä¸ºï¼š" + str(result))
    avg_result = result / 89
    print("email-dept3ç½‘ç»œçš„å¹³å‡èšç±»ç³»æ•°å’Œä¸ºï¼š" + str(avg_result))
    print(round(avg_result, 4))
    r = nx.degree_pearson_correlation_coefficient(G, weight=None, nodes=None)
    print("email-dept3ç½‘ç»œçš„åŒé…ç³»æ•°ä¸ºï¼š" + str(r))
    print(round(r, 4))


def diameter(G):
    mapping = {old_label: new_label for new_label, old_label in enumerate(G.nodes())}
    H = nx.relabel_nodes(G, mapping)
    maxh1 = max(H) + 1
    A = np.zeros((maxh1, maxh1))
    for i in range(maxh1):
        for j in range(maxh1):
            if nx.has_path(H, i, j) == True:
                A[i][j] = nx.shortest_path_length(H, i, j)
            else:
                A[i][j] = 0
    print('å›¾çš„ç›´å¾„ï¼š', A.max())


if __name__ == '__main__':
    G = get_Graph3('email-Eu-core-temporal-Dept3.txt')
    # print_graph(G)

    start_cpu_time = time.process_time()
    start_time = time.time()

    kshell(G)
    kshell2(G)
    sort_kshell()
    get_Salton(G)
    bc_cc(G)
    neighbours_bc(G)

    importance_node()
    zhenghe_data()
    shuju_quzao()
    core_fringe_node()

    end_cpu_time = time.process_time()
    cpu_time = end_cpu_time - start_cpu_time
    print("CPUæ—¶é—´:", cpu_time)

    end_time = time.time()
    execution_time = end_time - start_time
    print("æ‰§è¡Œæ—¶é—´:", execution_time)

    cpu_load = psutil.cpu_percent(interval=1)
    print("CPUè´Ÿè½½:", cpu_load)
    # Monotonicity()
    # diameter(G)
    # core_fringe_node()
